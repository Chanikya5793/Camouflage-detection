{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-18T07:15:43.387606Z","iopub.status.busy":"2023-04-18T07:15:43.386831Z","iopub.status.idle":"2023-04-18T07:16:17.329319Z","shell.execute_reply":"2023-04-18T07:16:17.328281Z","shell.execute_reply.started":"2023-04-18T07:15:43.387569Z"},"trusted":true},"outputs":[],"source":["!apt-get update && apt-get install -y python3-opencv\n","%pip install scikit-learn scipy matplotlib\n","\n","%pip install -q tensorflow-addons\n","%pip install -q tensorflow-probability\n","%pip install -q opencv-python-headless\n","%pip install -q seaborn\n","\n","\n","%pip install -qU wandb\n","%pip install -qU scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T07:16:17.331667Z","iopub.status.busy":"2023-04-18T07:16:17.331315Z","iopub.status.idle":"2023-04-18T07:16:31.109735Z","shell.execute_reply":"2023-04-18T07:16:31.108836Z","shell.execute_reply.started":"2023-04-18T07:16:17.331634Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import tensorflow as tf\n","\n","import os\n","import tensorflow_datasets as tfds\n","#import tensorflow_addons as tfa\n","import tensorflow_probability as tfp\n","\n","# CHANGED FOR TPU 1VM:\n","# Detect hardware, return appropriate distribution strategy\n","'''try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=\"local\") # \"local\" for 1VM TPU\n","    strategy = tf.distribute.TPUStrategy(tpu)\n","except tf.errors.NotFoundError:\n","    strategy = tf.distribute.MirroredStrategy()\n","    \n","print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n","'''\n","#resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n","#tf.config.experimental_connect_to_cluster(resolver)\n","# This is the TPU initialization code that has to be at the beginning.\n","#tf.tpu.experimental.initialize_tpu_system(resolver)\n","#print(\"All devices: \", tf.config.list_logical_devices('TPU'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T07:19:58.761329Z","iopub.status.busy":"2023-04-18T07:19:58.760309Z","iopub.status.idle":"2023-04-18T07:19:58.767034Z","shell.execute_reply":"2023-04-18T07:19:58.765710Z","shell.execute_reply.started":"2023-04-18T07:19:58.761283Z"},"trusted":true},"outputs":[],"source":["import cv2\n","def feature(image):\n","\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","    return image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T07:47:58.611058Z","iopub.status.busy":"2023-04-18T07:47:58.610382Z","iopub.status.idle":"2023-04-18T07:47:58.616437Z","shell.execute_reply":"2023-04-18T07:47:58.615442Z","shell.execute_reply.started":"2023-04-18T07:47:58.611020Z"},"trusted":true},"outputs":[],"source":["height = 512\n","width = 512\n","font_size = 20\n","def apply_visual_attention(path):\n","    img = cv2.imread(path, 0)\n","#     img = cv2.imread(path)\n","#     resized_img = cv2.resize(img, (height, width))\n","#     denoised_img = cv2.medianBlur(resized_img, 5)\n","#     th = cv2.adaptiveThreshold(denoised_img, maxValue = 255, adaptiveMethod = cv2.ADAPTIVE_THRESH_GAUSSIAN_C, thresholdType = cv2.THRESH_BINARY, blockSize = 11, C = 2)\n","    return img"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T08:06:45.330141Z","iopub.status.busy":"2023-04-18T08:06:45.329118Z"},"trusted":true},"outputs":[],"source":["\n","import os\n","import cv2\n","import numpy as np\n","\n","# Set the path to the dataset folders\n","train_image_path = '/kaggle/input/cod10k/COD10K-v3/Train/Image/'\n","train_gt_path = '/kaggle/input/cod10k/COD10K-v3/Train/GT_Object/'\n","test_image_path = '/kaggle/input/cod10k/COD10K-v3/Test/Image/'\n","test_gt_path = '/kaggle/input/cod10k/COD10K-v3/Test/GT_Object/'\n","\n","\n","img_size2=(512,512)\n","\n","\n","# Function to load images and ground truth instances\n","def load_data(image_path, gt_path,maxi):\n","    images = []\n","    gt_instances = []\n","    c=0\n","    for filename in sorted(os.listdir(image_path)):\n","        # Check if file is a JPG image\n","        c+=1\n","        if(c>=maxi):\n","            break\n","        if(c%50==0):\n","            print(c)\n","        if filename.endswith('.jpg'):\n","            # Load image and resize to (256,256)\n","#             img = cv2.imread(image_path + filename)\n","            img=apply_visual_attention(image_path + filename)\n","            img = cv2.resize(img, img_size2)\n","            images.append(img)\n","            # Load ground truth instance and resize to (256,256)\n","            gt = cv2.imread(gt_path + filename[:-4] + '.png', cv2.IMREAD_GRAYSCALE)\n","            gt = cv2.resize(gt, img_size2, interpolation=cv2.INTER_LINEAR)\n","            gt_instances.append(gt)\n","    return np.array(images), np.array(gt_instances)\n","\n","\n","# Load training data\n","train_images, train_gt_instances = load_data(train_image_path, train_gt_path,6000)\n","\n","# Load testing data\n","test_images, test_gt_instances = load_data(test_image_path, test_gt_path,2000)\n","\n","print('done')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T07:17:19.096057Z","iopub.status.busy":"2023-04-18T07:17:19.095753Z","iopub.status.idle":"2023-04-18T07:17:19.100569Z","shell.execute_reply":"2023-04-18T07:17:19.099720Z","shell.execute_reply.started":"2023-04-18T07:17:19.096031Z"},"trusted":true},"outputs":[],"source":["print('hello')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T07:52:49.692297Z","iopub.status.busy":"2023-04-18T07:52:49.690970Z","iopub.status.idle":"2023-04-18T07:52:50.249152Z","shell.execute_reply":"2023-04-18T07:52:50.247728Z","shell.execute_reply.started":"2023-04-18T07:52:49.692247Z"},"trusted":true},"outputs":[],"source":["# from tensorflow.keras.models import Sequential\n","# from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dropout\n","# from tensorflow.keras.optimizers import Adam\n","\n","# from tensorflow.keras import backend\n","# backend.clear_session()\n","\n","\n","# def build_generator(input_shape):\n","#     model = Sequential()\n","#     # Encoder\n","#     model.add(Conv2D(64, kernel_size=(3, 3), strides=(2, 2), input_shape=input_shape, padding='same', activation='relu'))\n","#     model.add(Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu'))\n","#     model.add(Conv2D(256, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu'))\n","#     model.add(Dropout(0.5))\n","#     # Decoder\n","#     model.add(Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', activation='relu'))\n","#     model.add(Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu'))\n","#     model.add(Conv2DTranspose(1, (3, 3), strides=(2, 2), padding='same', activation='sigmoid'))\n","    \n","#     # Output\n","#     model.add(Conv2D(1, (1, 1), activation='sigmoid'))\n","\n","#     return model\n","\n","\n","\n","# with strategy.scope():\n","#     input_shape = (512,512,3)\n","#     generator = build_generator(input_shape)\n","    \n","#     # Define optimizer and loss function\n","#     optimizer = tf.keras.optimizers.Adam(lr=0.001)\n","#     loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","\n","#     # Compile the model\n","#     generator.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n","    \n","#     generator.summary()\n","\n","\n","\n","\n","import tensorflow\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dropout\n","from tensorflow.keras.optimizers import Adam\n","\n","from tensorflow.keras import backend\n","backend.clear_session()\n","\n","\n","def build_generator(input_shape):\n","    model = Sequential()\n","    # Encoder\n","    model.add(Conv2D(64, kernel_size=(3, 3), strides=(2, 2), input_shape=input_shape, padding='same', activation='relu'))\n","    model.add(Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu'))\n","    model.add(Conv2D(256, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu'))\n","    model.add(Conv2D(512, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu'))\n","    model.add(Dropout(0.5))\n","    # Decoder\n","    model.add(Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', activation='relu'))\n","    model.add(Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', activation='relu'))\n","    model.add(Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu'))\n","    model.add(Conv2DTranspose(1, (3, 3), strides=(2, 2), padding='same', activation='sigmoid'))\n","    \n","    # Output\n","    model.add(Conv2D(1, (1, 1), activation='sigmoid'))\n","\n","    return model\n","\n","\n","\n","with strategy.scope():\n","    input_shape = (512,512,1)\n","    generator = build_generator(input_shape)\n","    \n","    # Define optimizer and loss function\n","    optimizer = tf.keras.optimizers.Adam(lr=0.001)\n","    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","\n","    # Compile the model\n","    generator.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy', tf.keras.metrics.Precision()])\n","    \n","    generator.summary()\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# #---- managable output with all images (520,720,  3) for 50 epochs, bsize 32\n","\n","# from tensorflow.keras.models import Sequential\n","# from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dropout\n","# from tensorflow.keras.optimizers import Adam\n","# from tensorflow.keras import backend\n","# from tensorflow.keras.losses import binary_crossentropy\n","\n","# backend.clear_session()\n","\n","# from tensorflow.keras.models import Model\n","# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, concatenate, UpSampling2D\n","\n","# # Define the U-Net model\n","# def unet(input_shape=(512, 512, 3)):\n","#     inputs = Input(input_shape)\n","    \n","#     # Encoder\n","#     conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n","#     conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n","#     pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","#     conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n","#     conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n","#     pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","#     conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n","#     conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n","#     pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","    \n","#     # Decoder\n","#     up4 = UpSampling2D(size=(2, 2))(pool3)\n","#     conv4 = Conv2D(256, 2, activation='relu', padding='same')(up4)\n","#     merge4 = concatenate([conv3, conv4], axis=3)\n","#     conv4 = Conv2D(256, 3, activation='relu', padding='same')(merge4)\n","#     conv4 = Conv2D(256, 3, activation='relu', padding='same')(conv4)\n","#     up5 = UpSampling2D(size=(2, 2))(conv4)\n","#     conv5 = Conv2D(128, 2, activation='relu', padding='same')(up5)\n","#     merge5 = concatenate([conv2, conv5], axis=3)\n","#     conv5 = Conv2D(128, 3, activation='relu', padding='same')(merge5)\n","#     conv5 = Conv2D(128, 3, activation='relu', padding='same')(conv5)\n","#     up6 = UpSampling2D(size=(2, 2))(conv5)\n","#     conv6 = Conv2D(64, 2, activation='relu', padding='same')(up6)\n","#     merge6 = concatenate([conv1, conv6], axis=3)\n","#     conv6 = Conv2D(64, 3, activation='relu', padding='same')(merge6)\n","#     conv6 = Conv2D(64, 3, activation='relu', padding='same')(conv6)\n","\n","#     # Output layer\n","#     outputs = Conv2D(3, 1, activation='sigmoid')(conv6)\n","\n","#     # Define the model\n","#     model = Model(inputs=inputs, outputs=outputs)\n","\n","#     return model\n","\n","\n","\n","# with strategy.scope():\n","#     input_shape = (512,512, 3)\n","#     generator = unet(input_shape)\n","    \n","#     optimizer = tf.keras.optimizers.Adam(lr=0.001)\n","    \n","#     loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","\n","#     generator.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy', tf.keras.metrics.Precision()])\n","\n","#     generator.summary()\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T07:49:13.298793Z","iopub.status.busy":"2023-04-18T07:49:13.297904Z","iopub.status.idle":"2023-04-18T07:49:13.304571Z","shell.execute_reply":"2023-04-18T07:49:13.303523Z","shell.execute_reply.started":"2023-04-18T07:49:13.298742Z"},"trusted":true},"outputs":[],"source":["train_images[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T07:49:11.800866Z","iopub.status.busy":"2023-04-18T07:49:11.800122Z","iopub.status.idle":"2023-04-18T07:49:11.806919Z","shell.execute_reply":"2023-04-18T07:49:11.805813Z","shell.execute_reply.started":"2023-04-18T07:49:11.800832Z"},"trusted":true},"outputs":[],"source":["train_gt_instances[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T07:52:54.648111Z","iopub.status.busy":"2023-04-18T07:52:54.647011Z","iopub.status.idle":"2023-04-18T07:55:24.252268Z","shell.execute_reply":"2023-04-18T07:55:24.250894Z","shell.execute_reply.started":"2023-04-18T07:52:54.648071Z"},"trusted":true},"outputs":[],"source":["num_epochs = 25\n","batch_size = 32\n","\n","# Train the model\n","history = generator.fit(train_images, train_gt_instances,\n","                        epochs=num_epochs,\n","                        batch_size=batch_size,\n","                        validation_data=(test_images, test_gt_instances),\n","#                         reset_metrics=True\n","                       )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-18T07:19:57.937914Z","iopub.status.idle":"2023-04-18T07:19:57.938304Z","shell.execute_reply":"2023-04-18T07:19:57.938129Z","shell.execute_reply.started":"2023-04-18T07:19:57.938109Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# import numpy as np\n","# from keras.models import Model\n","# from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, Activation\n","\n","# def residual_block(input_layer, num_filters, kernel_size=(3,3), strides=(1,1), padding='same'):\n","#     x = Conv2D(num_filters, kernel_size, strides=strides, padding=padding)(input_layer)\n","#     x = BatchNormalization()(x)\n","#     x = Activation('relu')(x)\n","#     x = Conv2D(num_filters, kernel_size, strides=strides, padding=padding)(x)\n","#     x = BatchNormalization()(x)\n","#     x = Activation('relu')(x)\n","#     shortcut = Conv2D(num_filters, kernel_size=(1,1), strides=strides, padding=padding)(input_layer)\n","#     shortcut = BatchNormalization()(shortcut)\n","#     x = concatenate([x, shortcut])\n","#     x = Activation('relu')(x)\n","#     return x\n","\n","# def sinet(input_shape=(256, 256, 3)):\n","#     inputs = Input(input_shape)\n","\n","#     # Encoding Path\n","#     conv1 = residual_block(inputs, 64)\n","#     pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","\n","#     conv2 = residual_block(pool1, 128)\n","#     pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","\n","#     conv3 = residual_block(pool2, 256)\n","#     pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","\n","#     # Decoding Path\n","#     up4 = UpSampling2D(size=(2, 2))(pool3)\n","#     conv4 = residual_block(up4, 256)\n","#     concat4 = concatenate([conv4, conv3], axis=-1)\n","\n","#     up5 = UpSampling2D(size=(2, 2))(concat4)\n","#     conv5 = residual_block(up5, 128)\n","#     concat5 = concatenate([conv5, conv2], axis=-1)\n","\n","#     up6 = UpSampling2D(size=(2, 2))(concat5)\n","#     conv6 = residual_block(up6, 64)\n","#     concat6 = concatenate([conv6, conv1], axis=-1)\n","\n","#     # Output\n","#     outputs = Conv2D(1, (1, 1), activation='sigmoid')(concat6)\n","\n","#     # Model definition\n","#     model = Model(inputs=[inputs], outputs=[outputs])\n","#     return model\n","\n","# from tensorflow.keras import backend\n","# backend.clear_session()\n","\n","# with strategy.scope():\n","#     input_shape = (256,256,1)\n","#     generator = sinet(input_shape)\n","\n","#     # Define optimizer and loss function\n","#     optimizer = tf.keras.optimizers.Adam(lr=0.0005)\n","#     loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","\n","#     # Compile the model\n","#     generator.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy', tf.keras.metrics.Precision()])\n","# #     generator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","#     generator.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T07:55:45.346047Z","iopub.status.busy":"2023-04-18T07:55:45.345629Z","iopub.status.idle":"2023-04-18T08:02:00.880046Z","shell.execute_reply":"2023-04-18T08:02:00.878614Z","shell.execute_reply.started":"2023-04-18T07:55:45.346014Z"},"trusted":true},"outputs":[],"source":["num_epochs = 100\n","batch_size = 64\n","\n","# Train the model\n","history = generator.fit(train_images, train_gt_instances,\n","                        epochs=num_epochs,\n","                        batch_size=batch_size,\n","                        validation_data=(test_images, test_gt_instances),\n","#                         reset_metrics=True\n","                       )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-18T07:19:57.941690Z","iopub.status.idle":"2023-04-18T07:19:57.942113Z","shell.execute_reply":"2023-04-18T07:19:57.941926Z","shell.execute_reply.started":"2023-04-18T07:19:57.941904Z"},"trusted":true},"outputs":[],"source":["history2 = generator.fit(train_images, train_gt_instances,\n","                        epochs=num_epochs,\n","                        batch_size=batch_size,\n","                        validation_data=(test_images, test_gt_instances),\n","#                         reset_metrics=True\n","                       )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T08:05:38.279309Z","iopub.status.busy":"2023-04-18T08:05:38.278291Z","iopub.status.idle":"2023-04-18T08:05:39.363594Z","shell.execute_reply":"2023-04-18T08:05:39.362252Z","shell.execute_reply.started":"2023-04-18T08:05:38.279268Z"},"trusted":true},"outputs":[],"source":["\n","import matplotlib.pyplot as plt\n","img_path='/kaggle/input/cod10k/COD10K-v3/Train/Image/COD10K-CAM-1-Aquatic-12-Pagurian-480.jpg'\n","otimg = cv2.imread(img_path)\n","otimg=apply_visual_attention(img_path)\n","timg = cv2.resize(otimg, img_size2)\n","\n","\n","plt.imshow(timg)\n","\n","timg=np.array([timg])\n","print(timg.shape)\n","predictions = generator.predict(timg)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T08:05:48.924682Z","iopub.status.busy":"2023-04-18T08:05:48.923527Z","iopub.status.idle":"2023-04-18T08:05:49.193300Z","shell.execute_reply":"2023-04-18T08:05:49.191992Z","shell.execute_reply.started":"2023-04-18T08:05:48.924630Z"},"trusted":true},"outputs":[],"source":["plt.imshow(predictions[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T08:04:51.604541Z","iopub.status.busy":"2023-04-18T08:04:51.603506Z","iopub.status.idle":"2023-04-18T08:04:51.915196Z","shell.execute_reply":"2023-04-18T08:04:51.913874Z","shell.execute_reply.started":"2023-04-18T08:04:51.604493Z"},"trusted":true},"outputs":[],"source":["# timg='/kaggle/input/cod10k/COD10K-v3/Test/Image/COD10K-CAM-1-Aquatic-13-Pipefish-528.jpg'\n","\n","timg= img_path.split('/')\n","\n","timgp=timg.pop().split('.')\n","timgp=[timgp[0],'png']\n","timgp=\".\".join(timgp)\n","timg[6]='GT_Object'\n","\n","timg=timg+[timgp]\n","timg='/'.join(timg)\n","print(timg)\n","timg = cv2.resize(cv2.imread(timg), img_size2)\n","# timg=feature(timg)\n","plt.imshow(timg)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-18T07:19:57.947643Z","iopub.status.idle":"2023-04-18T07:19:57.948000Z","shell.execute_reply":"2023-04-18T07:19:57.947848Z","shell.execute_reply.started":"2023-04-18T07:19:57.947830Z"},"trusted":true},"outputs":[],"source":["# import cv2\n","# import numpy as np\n","# import matplotlib.pyplot as plt\n","\n","# # Load the image\n","# img = cv2.imread('/kaggle/input/cod10k/COD10K-v3/Train/Image/COD10K-CAM-1-Aquatic-1-BatFish-1.jpg')\n","\n","# # Convert the image to the LMS color space\n","# lms = cv2.cvtColor(img, cv2.COLOR_BGR2LMS)\n","\n","# # Define the range of colors to extract (in LMS)\n","# lower_range = np.array([0.1, 0.1, 0.1])\n","# upper_range = np.array([0.9, 0.9, 0.9])\n","\n","# # Threshold the image to extract the specified colors\n","# mask = cv2.inRange(lms, lower_range, upper_range)\n","\n","# # Compute the color histogram\n","# hist = cv2.calcHist([img], [0], mask, [256], [0, 256])\n","\n","# # Normalize the histogram\n","# hist = cv2.normalize(hist, None)\n","\n","# # Create a black image\n","# hist_img = np.zeros((256, 256, 3), np.uint8)\n","\n","# # Draw the histogram on the image\n","# for i in range(256):\n","#     x1 = i\n","#     y1 = 255\n","#     x2 = i\n","#     y2 = 255 - int(hist[i] * 255)\n","#     cv2.line(hist_img, (x1, y1), (x2, y2), (255, 255, 255), 1)\n","\n","# # Display the histogram image\n","# plt.imshow('Histogram')\n","# cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-18T07:19:57.949896Z","iopub.status.idle":"2023-04-18T07:19:57.950240Z","shell.execute_reply":"2023-04-18T07:19:57.950080Z","shell.execute_reply.started":"2023-04-18T07:19:57.950062Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dropout, Flatten, Dense\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications.vgg16 import VGG16\n","\n","# Clear backend session\n","from tensorflow.keras import backend\n","backend.clear_session()\n","\n","# Set input shape for images\n","input_shape = (512, 512, 3)\n","\n","# Define function to build generator model\n","def build_generator(input_shape):\n","    # Load pre-trained VGG16 model for feature extraction\n","    vgg16_model = VGG16(input_shape=input_shape, include_top=False, weights='imagenet')\n","    vgg16_model.trainable = False\n","\n","    # Create generator model\n","    model = Sequential()\n","    model.add(vgg16_model)\n","    model.add(Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', activation='relu'))\n","    model.add(Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu'))\n","    model.add(Conv2DTranspose(1, (3, 3), strides=(2, 2), padding='same', activation='sigmoid'))\n","    model.add(Conv2D(1, (1, 1), activation='sigmoid'))\n","\n","    return model\n","\n","# Define function to build discriminator model\n","def build_discriminator(input_shape):\n","    model = Sequential()\n","    model.add(Conv2D(64, kernel_size=(3, 3), strides=(2, 2), input_shape=input_shape, padding='same', activation='relu'))\n","    model.add(Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu'))\n","    model.add(Conv2D(256, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu'))\n","    model.add(Flatten())\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","    return model\n","\n","# Define function to train the GAN\n","def train_gan(generator, discriminator, combined, x_train, batch_size, epochs):\n","    # Define loss functions\n","    adversarial_loss_fn = tf.keras.losses.BinaryCrossentropy()\n","    generator_loss_fn = tf.keras.losses.MeanSquaredError()\n","\n","    # Define optimizers\n","    adversarial_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n","    generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n","\n","    # Train the GAN\n","    for epoch in range(epochs):\n","        # Train discriminator\n","        idx = np.random.randint(0, x_train.shape[0], batch_size)\n","        real_images = x_train[idx]\n","        fake_images = generator.predict(real_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-18T07:19:57.951574Z","iopub.status.idle":"2023-04-18T07:19:57.951998Z","shell.execute_reply":"2023-04-18T07:19:57.951811Z","shell.execute_reply.started":"2023-04-18T07:19:57.951790Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","fig = plt.figure(figsize=(17, 7))\n","\n","# define interval ranges for hue, saturation, and lightness\n","hue_ranges = [(0, 270), (20, 45), (45, 55), (55, 80), (80, 100), (100, 120),\n","            (120, 140), (140, 165), (165, 190), (190, 220), (220, 255), (255, 270)]\n","sat_ranges = [(0.0, 0.1), (0.1, 0.25), (0.25, 0.45), (0.45, 0.65), (0.65, 0.85), (0.85, 1)]\n","val_ranges = [(0.0, 0.15), (0.15, 0.4), (0.4, 0.6), (0.6, 0.8), (0.8, 1.0)]\n","\n","# define function to quantize HSV image based on interval ranges\n","def quantize_hsv(img):\n","    # convert image to HSV color space\n","    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n","    # quantize hue, saturation, and lightness separately\n","    h, s, v = cv2.split(hsv_img)\n","    h_quantized = np.zeros_like(h)\n","    s_quantized = np.zeros_like(s)\n","    v_quantized = np.zeros_like(v)\n","    for i, (h_range, s_range, v_range) in enumerate(zip(hue_ranges, sat_ranges, val_ranges)):\n","        h_quantized[((h >= h_range[0]) & (h < h_range[1]))] = i\n","        s_quantized[((s >= s_range[0]) & (s < s_range[1]))] = i\n","        v_quantized[((v >= v_range[0]) & (v < v_range[1]))] = i\n","    # combine quantized hue, saturation, and lightness to form Q\n","    Q = 25 * h_quantized + 5 * s_quantized + v_quantized\n","    return Q\n","\n","# define function to generate color histogram of an image\n","def color_histogram(img):\n","    # quantize image using HSV color space\n","    Q = quantize_hsv(img)\n","    \n","    # showing image\n","    fig.add_subplot(1, 2, 1)\n","    plt.imshow(Q)\n","    plt.axis('off')\n","    plt.title(\"Quantize_Image_Output\")\n","    \n","    # compute color histogram of quantized image\n","    hist, _ = np.histogram(Q.ravel(), bins=range(301))\n","    # normalize histogram\n","    hist = hist.astype(float) / float(img.size)\n","    return hist\n","\n","# load input image\n","img = cv2.imread('/kaggle/input/cod10k/COD10K-v3/Train/Image/COD10K-CAM-1-Aquatic-1-BatFish-1.jpg')\n","\n","# generate color histogram of input image\n","hist = color_histogram(img)\n","\n","Q = quantize_hsv(img)\n","\n","# Adds a subplot at the 2nd position\n","fig.add_subplot(1, 2, 2)\n","# plot histogram\n","plt.bar(range(300), hist)\n","plt.title(\"Histogram of Quantized Image\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-18T07:19:57.952878Z","iopub.status.idle":"2023-04-18T07:19:57.953198Z","shell.execute_reply":"2023-04-18T07:19:57.953051Z","shell.execute_reply.started":"2023-04-18T07:19:57.953034Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","# define the increment function of lightness visual attention\n","def fv(v, kv=1.0):\n","    return kv * v**0.6\n","\n","# define the increment function of hue visual attention\n","def fh(h):\n","    if np.mod(h, np.pi/3) == 0:\n","        return 1\n","    else:\n","        return np.cos(np.mod(h, np.pi/3) - (np.pi/6)) / np.cos(np.pi/6)\n","\n","# define the increment function of chroma visual attention\n","def fc(h, s, v, kv=1.0, kc=1.0):\n","    return kv * fv(v, kv) * fh(h) * s**0.6 * kc\n","\n","# define the function of color visual attention\n","def f(h, s, v, kv=1.0, kc=1.0):\n","    return 1 + fv(v, kv) + fc(h, s, v, kv, kc)\n","\n","\n","# convert the image to HSV color space\n","hsv_img = img\n","\n","# initialize an empty matrix for the color visual attention map\n","cva_map = np.zeros_like(hsv_img[:, :, 0], dtype=float)\n","\n","# iterate over each pixel in the image and calculate its color visual attention\n","for i in range(hsv_img.shape[0]):\n","    for j in range(hsv_img.shape[1]):\n","        h, s, v = hsv_img[i, j]\n","        cva = f(h, s, v)\n","        cva_map[i, j] = cva\n","\n","# normalize the color visual attention map to the range [0, 255]\n","cva_map = cv2.normalize(cva_map, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n","\n","# display the color visual attention map\n","plt.imshow(cv2.applyColorMap(cva_map, cv2.COLORMAP_JET))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-18T07:19:57.954173Z","iopub.status.idle":"2023-04-18T07:19:57.954556Z","shell.execute_reply":"2023-04-18T07:19:57.954350Z","shell.execute_reply.started":"2023-04-18T07:19:57.954333Z"},"trusted":true},"outputs":[],"source":["#import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","fig = plt.figure(figsize=(17, 7))\n","\n","\n","# define function to generate color histogram of an image\n","def color_histogram_percp(img):\n","    # showing image\n","    fig.add_subplot(1, 2, 1)\n","    plt.imshow(cva_map)\n","    plt.axis('off')\n","    plt.title(\"CVA Image\")\n","    \n","    # compute color histogram of quantized image\n","    hist, _ = np.histogram(cva_map.ravel(), bins=range(301))\n","    # normalize histogram\n","    hist = hist.astype(float) / float(img.size)\n","    return hist\n","\n","hist2 = color_histogram_percp(img)\n","\n","# Adds a subplot at the 2nd position\n","fig.add_subplot(1, 2, 2)\n","# plot histogram\n","plt.bar(range(300), hist2)\n","plt.title(\"Histogram of CVA Image\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-18T07:19:57.956287Z","iopub.status.idle":"2023-04-18T07:19:57.956628Z","shell.execute_reply":"2023-04-18T07:19:57.956471Z","shell.execute_reply.started":"2023-04-18T07:19:57.956454Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","def calculate_color_feature(image, mask):\n","    # convert the image to the HSV color space\n","    hsv_img = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","    # define the visual attention function\n","    def f(h, s, v):\n","        return np.exp(-(h / 16)**2) * np.exp(-(s / 255)**2) * np.exp(-(v / 255)**2)\n","    # calculate the color histogram\n","    color_hist = np.zeros((32768))\n","    visual_attention = np.zeros((128, 16, 16))\n","    for i in range(0, 256, 8):\n","        for j in range(0, 256, 8):\n","            for k in range(0, 256, 8):\n","                h, s, v = cv2.split(cv2.merge([np.array([i]), np.array([j]), np.array([k])]))\n","                ci = i * 25 + j * 5 + k\n","                visual_attention[ci] = f(h, s, v)\n","    # normalize the color histogram based on visual perception\n","    visual_attention = visual_attention / visual_attention.sum()\n","    color_hist = visual_attention.reshape(-1) * color_hist\n","    # calculate the color feature of the region Dk\n","    h = np.sum(color_hist * hsv_img[:, :, 0][mask]) / np.sum(color_hist)\n","    s = np.sum(color_hist * hsv_img[:, :, 1][mask]) / np.sum(color_hist)\n","    v = np.sum(color_hist * hsv_img[:, :, 2][mask]) / np.sum(color_hist)\n","    return (h, s, v)\n","\n","\n","\n","# create the mask for the region Dk\n","mask = np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)\n","mask[100:200, 100:200] = 255\n","\n","# calculate the color feature of the region Dk\n","color_feature = calculate_color_feature(img, mask)\n","\n","# display the color feature\n","print(\"Color feature:\", color_feature)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-18T07:19:57.957626Z","iopub.status.idle":"2023-04-18T07:19:57.957976Z","shell.execute_reply":"2023-04-18T07:19:57.957822Z","shell.execute_reply.started":"2023-04-18T07:19:57.957804Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","fig = plt.figure(figsize=(17, 7))\n","\n","# define interval ranges for hue, saturation, and lightness\n","hue_ranges = [(0, 270), (20, 45), (45, 55), (55, 80), (80, 100), (100, 120), \n","              (120, 140), (140, 165), (165, 190), (190, 220), (220, 255), (255, 270)]\n","sat_ranges = [(0.0, 0.1), (0.1, 0.25), (0.25, 0.45), (0.45, 0.65), (0.65, 0.85), (0.85, 1)]\n","val_ranges = [(0.0, 0.15), (0.15, 0.4), (0.4, 0.6), (0.6, 0.8), (0.8, 1.0)]\n","\n","# define function to quantize HSV image based on interval ranges\n","def quantize_hsv(img):\n","    # convert image to HSV color space\n","    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n","    # quantize hue, saturation, and lightness separately\n","    h, s, v = cv2.split(hsv_img)\n","    h_quantized = np.zeros_like(h)\n","    s_quantized = np.zeros_like(s)\n","    v_quantized = np.zeros_like(v)\n","    for i, (h_range, s_range, v_range) in enumerate(zip(hue_ranges, sat_ranges, val_ranges)):\n","        h_quantized[((h >= h_range[0]) & (h < h_range[1]))] = i\n","        s_quantized[((s >= s_range[0]) & (s < s_range[1]))] = i\n","        v_quantized[((v >= v_range[0]) & (v < v_range[1]))] = i\n","    # combine quantized hue, saturation, and lightness to form Q\n","    Q = 25 * h_quantized + 5 * s_quantized + v_quantized\n","    return Q, hsv_img\n","\n","# define function to generate color histogram of an image\n","def color_histogram(img):\n","    # quantize image using HSV color space\n","    Q, _ = quantize_hsv(img)\n","    \n","    # showing image\n","    fig.add_subplot(1, 2, 1)\n","    plt.imshow(Q)\n","    plt.axis('off')\n","    plt.title(\"Quantize_Image_Output\")\n","    \n","    # compute color histogram of quantized image\n","    hist, _ = np.histogram(Q.ravel(), bins=range(301))\n","    # normalize histogram\n","    hist = hist.astype(float) / float(img.size)\n","    return hist\n","\n","# define the increment function of lightness visual attention\n","def fv(v, kv=1.0):\n","    return kv * v**0.6\n","\n","# define the increment function of hue visual attention\n","def fh(h):\n","    if np.mod(h, np.pi/3) == 0:\n","        return 1\n","    else:\n","        return np.cos(np.mod(h, np.pi/3) - (np.pi/6)) / np.cos(np.pi/6)\n","\n","# define the increment function of chroma visual attention\n","def f(h, s, v, kv=1.0, kc=1.0):\n","    return 1 + fv(v, kv) + fc(h, s, v, kv, kc)\n","for j in range(hsv_img.shape[1]):\n","    h, s, v = hsv_img[i, j]\n","    cva = f(h, s, v)\n","    cva_map[i, j] = cva\n","\n","#normalize the color visual attention map to the range [0, 255]\n","cva_map = cv2.normalize(cva_map, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n","\n","#display the original image and the color visual attention map\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 10))\n","ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","ax1.set_title('Original Image')\n","ax1.axis('off')\n","ax2.imshow(cv2.applyColorMap(cva_map, cv2.COLORMAP_JET))\n","ax2.set_title('Color Visual Attention Map')\n","ax2.axis('off')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-18T07:19:57.959346Z","iopub.status.idle":"2023-04-18T07:19:57.959687Z","shell.execute_reply":"2023-04-18T07:19:57.959526Z","shell.execute_reply.started":"2023-04-18T07:19:57.959508Z"},"trusted":true},"outputs":[],"source":["%pip install tensorflow\n","# Import required libraries\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from keras.preprocessing.image import ImageDataGenerator\n","import numpy as np\n","from tensorflow.keras.utils import load_img,img_to_array\n","from PIL import Image\n","\n","\n","def load_mask( image_path):\n","    # Load the original image\n","    img = Image.open(image_path)\n","    \n","    # Resize the image to the same shape as the input images\n","    img = img.resize((512,512), resample=Image.BILINEAR)\n","    \n","    # Convert the image to grayscale and then to a binary mask\n","    img = img.convert('L')\n","    img = np.array(img)\n","    img[img > 0] = 1\n","    img = np.expand_dims(img, axis=-1)\n","    \n","    return img\n","\n","# Import the necessary module\n","from tensorflow.distribute import MirroredStrategy\n","\n","# Create a MirroredStrategy.\n","strategy = MirroredStrategy()\n","with strategy.scope():\n","    # Define paths to train and test data\n","#     train_data_dir = '/kaggle/input/cod10k/COD10K-v3/Train/Image'\n","#     test_data_dir = '/kaggle/input/cod10k/COD10K-v3/Test/Image'\n","\n","#     # Define path to ground truth file\n","#     train_ground_truth_dir = '/kaggle/input/cod10k/COD10K-v3/Train/GT_Object'\n","#     test_ground_truth_dir = '/kaggle/input/cod10k/COD10K-v3/Test/GT_Object'\n","\n","    \n","#     # Define image parameters\n","    img_width, img_height = 512, 512\n","    input_shape = (img_width, img_height, 3)\n","\n","#     # Define batch size and number of epochs\n","#     batch_size = 32\n","#     epochs = 10\n","\n","#     # Define image generators with data augmentation for training set\n","#     train_datagen = ImageDataGenerator(\n","#         rescale=1./255,\n","#         shear_range=0.2,\n","#         zoom_range=0.2,\n","#         horizontal_flip=True)\n","\n","#     # Define image generator without data augmentation for test set\n","#     test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","#     # Load original images and ground truth images separately for the training set\n","#     train_images = []\n","#     train_ground_truths = []\n","\n","#     for file_name in os.listdir(train_data_dir):\n","#         # Load original image\n","#         img = load_img(os.path.join(train_data_dir, file_name), target_size=(img_width, img_height))\n","#         img_arr = img_to_array(img)\n","#         train_images.append(img_arr)\n","\n","#         # Load ground truth image\n","#         gt_img = load_img(os.path.join(train_ground_truth_dir, file_name[:-4] + '.png'), target_size=(img_width, img_height), color_mode='grayscale')\n","#         gt_img_arr = img_to_array(gt_img)\n","#         gt_img_arr /= 255.0\n","#         train_ground_truths.append(gt_img_arr)\n","\n","#     # Convert training data to numpy arrays\n","#     train_images = np.array(train_images)\n","#     train_ground_truths = np.array(train_ground_truths)\n","\n","#     # Combine original images and ground truth images for the training set\n","#     train_generator = train_datagen.flow(train_images, train_ground_truths, batch_size=batch_size)\n","\n","#     # Load original images and ground truth images separately for the test set\n","#     test_images = []\n","#     test_ground_truths = []\n","\n","#     for file_name in os.listdir(test_data_dir):\n","#         # Load original image\n","# #         img = load_img(os.path.join(test_data_dir, file_name), target_size=(img_width, img_height))\n","# #         img_arr = img_to_array(img)\n","# #         test_images.append(img_arr)\n","\n","# #         # Load ground truth image\n","# #         gt_img = load_img(os.path.join(test_ground_truth_dir, file_name[:-4] + '.png'), target_size=(img_width, img_height), color_mode='grayscale')\n","# #         gt_img_arr = img_to_array(gt_img)\n","# #         gt_img_arr /= 255.0\n","#         gt_img_arr=load_mask(os.path.join(test_data_dir, file_name))\n","#         test_ground_truths.append(gt_img_arr)\n","\n","#     # Convert test data to numpy arrays\n","#     test_images = np.array(test_images)\n","#     test_ground_truths = np.array(test_ground_truths)\n","\n","#     # Combine original images and ground truth images for the test set\n","#     test_generator = test_datagen.flow(test_images, test_ground_truths, batch_size=batch_size)\n","\n","    %pip install tensorflow\n","    # Freeze the pre-trained weights of the base model\n","    from tensorflow.keras.applications import MobileNetV2\n","\n","# Load MobileNetV2 as the base model\n","    base_model = MobileNetV2(weights='imagenet', include_top=False)\n","    base_model.trainable = False\n","\n","    # Add a custom head on top of the base model for fine-tuning\n","    inputs = keras.Input(shape=input_shape)\n","    x = base_model(inputs, training=False)\n","#     x = layers.GlobalAveragePooling2D()(x)\n","#     x = layers.Dense(128, activation='relu')(x)\n","    x = layers.Dropout(0.5)(x)\n","#     outputs = layers.Dense(1, activation='sigmoid')(x)\n","    outputs = Conv2D(1, (1, 1), activation='sigmoid')(x)\n","    model = keras.Model(inputs, outputs)\n","\n","    # Compile the model with binary cross-entropy loss and Adam optimizer\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","    # Create an ImageDataGenerator object\n","    datagen = ImageDataGenerator(rescale=1./255)\n","\n","    # Assuming train_dir is the path to the directory where your training images are\n","    train_generator = datagen.flow_from_directory(\n","        train_dir,\n","        target_size=(150, 150),\n","        batch_size=32,\n","        class_mode='binary')\n","\n","    # Train the model on the train data\n","    model.fit(train_generator, epochs=epochs, validation_data=test_generator)\n","    model.summary()\n","\n","history = model.fit(train_images, train_gt_instances,\n","                        epochs=10,\n","                        batch_size=32,\n","                        validation_data=(test_images, test_gt_instances),\n","                    )\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
